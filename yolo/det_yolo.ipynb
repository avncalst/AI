{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import requests\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"Tensorflow/workspace/datasets\"\n",
    "images_dir = os.path.join(root_dir, \"images\")\n",
    "annotations_dir = os.path.join(root_dir, \"annotations\")\n",
    "xml_dir = os.path.join(root_dir, \"xml_dir\")\n",
    "model_dir = os.path.join(root_dir, \"model_dir\")\n",
    "log_dir = os.path.join(root_dir, \"log\")\n",
    "yolo_dir = os.path.join(root_dir, \"yolo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelImg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(yolo_dir + \"/yolov8n.pt\")\n",
    "# results = model([\"testYolo.png\"]) \n",
    "# print(results)\n",
    "# Process results list\n",
    "# for result in results:\n",
    "#     boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "#     masks = result.masks  # Masks object for segmentation masks outputs\n",
    "#     keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "#     probs = result.probs  # Probs object for classification outputs\n",
    "#     obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "#     result.show()  # display to screen\n",
    "#     result.save(filename=\"result.jpg\")  # save to disk\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "# model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Open the video file\n",
    "# video_path = \"path/to/your/video/file.mp4\"image = cv2.imread(\"AnyPathToAnImage.jpeg\")\n",
    "# image = cv2.imread('testYolo.jpg')\n",
    "# # print(cv2.getBuildInformation())\n",
    "# cv2.imshow(\"Image\",image)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "video_path = \"oak-d.avi\"\n",
    "# video_path = \"output.avi\" \n",
    "# video_path = \"traffic.mp4\"\n",
    "# video_path = \"street.mp4\"\n",
    "\n",
    "print(video_path)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(50) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on custom data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1: View custom.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : /home/avncalst/Dropbox/deeplearning/SSD/RealTimeObjectDetection/Tensorflow/workspace/datasets/yolo/Training\n",
      "val : /home/avncalst/Dropbox/deeplearning/SSD/RealTimeObjectDetection/Tensorflow/workspace/datasets/yolo/Validation\n",
      "nc : 2\n",
      "names : {0: 'apple', 1: 'pear'}\n"
     ]
    }
   ],
   "source": [
    "# settings.yaml(/home/avncalst/.config/Ultralytics/settings.yaml) defines the datasets_dir and runs_dir:\n",
    "# datasets_dir: /home/avncalst/Dropbox/deeplearning/SSD/RealTimeObjectDetection/Tensorflow/workspace/datasets/yolo\n",
    "# /home/avncalst/Dropbox/deeplearning/SSD/RealTimeObjectDetection/Tensorflow/workspace/datasets/yolo/runs\n",
    "\n",
    "with open(yolo_dir +'/custom.yaml', 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "    # print(data)\n",
    "    for item, doc in data.items():\n",
    "        print(item, \":\", doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params:\n",
    "    #epochs=150\n",
    "    #batch=16 (default),-1:automatically adjust batch size for approximately 60% CUDA memory\n",
    "    #patience=0 (no early stopping)\n",
    "    #imgsz=640 (default), 416 (increases speed)\n",
    "\n",
    "# model = YOLO() # full custom \n",
    "# model.train(data=yolo_dir + \"/yolo.yaml\",epochs=150,batch=2) # results shown in yolo_dir/runs/detect/train\n",
    "model.train(data=yolo_dir + \"/custom.yaml\",epochs=150,patience=0,batch=12) # results shown in yolo_dir/runs/detect/train, no early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cust = YOLO(yolo_dir+\"/yolo_cust.pt\")\n",
    "# model_cust.predict(source=yolo_dir+\"/Validation/pear14.jpg\")\n",
    "results = model_cust([yolo_dir+\"/Validation/apple12.jpg\"]) \n",
    "print(results)\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "#     masks = result.masks  # Masks object for segmentation masks outputs\n",
    "#     keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "#     probs = result.probs  # Probs object for classification outputs\n",
    "#     obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=\"result.jpg\")  # save to disk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: View voc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path : ../datasets/VOC\n",
      "train : ['images/train2012', 'images/train2007', 'images/val2012', 'images/val2007']\n",
      "val : ['images/test2007']\n",
      "test : ['images/test2007']\n",
      "names : {0: 'aeroplane', 1: 'bicycle', 2: 'bird', 3: 'boat', 4: 'bottle', 5: 'bus', 6: 'car', 7: 'cat', 8: 'chair', 9: 'cow', 10: 'diningtable', 11: 'dog', 12: 'horse', 13: 'motorbike', 14: 'person', 15: 'pottedplant', 16: 'sheep', 17: 'sofa', 18: 'train', 19: 'tvmonitor'}\n",
      "download : import xml.etree.ElementTree as ET\n",
      "\n",
      "from tqdm import tqdm\n",
      "from ultralytics.utils.downloads import download\n",
      "from pathlib import Path\n",
      "\n",
      "def convert_label(path, lb_path, year, image_id):\n",
      "    def convert_box(size, box):\n",
      "        dw, dh = 1. / size[0], 1. / size[1]\n",
      "        x, y, w, h = (box[0] + box[1]) / 2.0 - 1, (box[2] + box[3]) / 2.0 - 1, box[1] - box[0], box[3] - box[2]\n",
      "        return x * dw, y * dh, w * dw, h * dh\n",
      "\n",
      "    in_file = open(path / f'VOC{year}/Annotations/{image_id}.xml')\n",
      "    out_file = open(lb_path, 'w')\n",
      "    tree = ET.parse(in_file)\n",
      "    root = tree.getroot()\n",
      "    size = root.find('size')\n",
      "    w = int(size.find('width').text)\n",
      "    h = int(size.find('height').text)\n",
      "\n",
      "    names = list(yaml['names'].values())  # names list\n",
      "    for obj in root.iter('object'):\n",
      "        cls = obj.find('name').text\n",
      "        if cls in names and int(obj.find('difficult').text) != 1:\n",
      "            xmlbox = obj.find('bndbox')\n",
      "            bb = convert_box((w, h), [float(xmlbox.find(x).text) for x in ('xmin', 'xmax', 'ymin', 'ymax')])\n",
      "            cls_id = names.index(cls)  # class id\n",
      "            out_file.write(\" \".join(str(a) for a in (cls_id, *bb)) + '\\n')\n",
      "\n",
      "\n",
      "# Download\n",
      "dir = Path(yaml['path'])  # dataset root dir\n",
      "url = 'https://github.com/ultralytics/assets/releases/download/v0.0.0/'\n",
      "urls = [f'{url}VOCtrainval_06-Nov-2007.zip',  # 446MB, 5012 images\n",
      "        f'{url}VOCtest_06-Nov-2007.zip',  # 438MB, 4953 images\n",
      "        f'{url}VOCtrainval_11-May-2012.zip']  # 1.95GB, 17126 images\n",
      "download(urls, dir=dir / 'images', curl=True, threads=3, exist_ok=True)  # download and unzip over existing paths (required)\n",
      "\n",
      "# Convert\n",
      "path = dir / 'images/VOCdevkit'\n",
      "for year, image_set in ('2012', 'train'), ('2012', 'val'), ('2007', 'train'), ('2007', 'val'), ('2007', 'test'):\n",
      "    imgs_path = dir / 'images' / f'{image_set}{year}'\n",
      "    lbs_path = dir / 'labels' / f'{image_set}{year}'\n",
      "    imgs_path.mkdir(exist_ok=True, parents=True)\n",
      "    lbs_path.mkdir(exist_ok=True, parents=True)\n",
      "\n",
      "    with open(path / f'VOC{year}/ImageSets/Main/{image_set}.txt') as f:\n",
      "        image_ids = f.read().strip().split()\n",
      "    for id in tqdm(image_ids, desc=f'{image_set}{year}'):\n",
      "        f = path / f'VOC{year}/JPEGImages/{id}.jpg'  # old img path\n",
      "        lb_path = (lbs_path / f.name).with_suffix('.txt')  # new label path\n",
      "        f.rename(imgs_path / f.name)  # move image\n",
      "        convert_label(path, lb_path, year, id)  # convert labels to YOLO format\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# path in voc.yaml  is relative to datasets_dir defined in settings.yaml\n",
    "\n",
    "with open(\"/home/avncalst/.local/lib/python3.10/site-packages/ultralytics/cfg/datasets/VOC.yaml\", 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "    # print(data)\n",
    "    for item, doc in data.items():\n",
    "        print(item, \":\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on  VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(yolo_dir + \"/yolov8n.pt\")\n",
    "model.train(data=\"/home/avncalst/.local/lib/python3.10/site-packages/ultralytics/cfg/datasets/VOC.yaml\", epochs=150, patience=0,imgsz=416)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference yolo_voc (416)\n",
    "\n",
    "training results are nomally saved in the folder \"runs\" at the same directory as the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(yolo_dir + \"/yolov8n_voc.pt\")\n",
    "# results = model([\"testYolo.png\"]) \n",
    "# print(results)\n",
    "# Process results list\n",
    "# for result in results:\n",
    "#     boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "#     masks = result.masks  # Masks object for segmentation masks outputs\n",
    "#     keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "#     probs = result.probs  # Probs object for classification outputs\n",
    "#     obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "#     result.show()  # display to screen\n",
    "#     result.save(filename=\"result.jpg\")  # save to disk\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "# model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Open the video file\n",
    "# video_path = \"path/to/your/video/file.mp4\"image = cv2.imread(\"AnyPathToAnImage.jpeg\")\n",
    "# image = cv2.imread('testYolo.jpg')\n",
    "# # print(cv2.getBuildInformation())\n",
    "# cv2.imshow(\"Image\",image)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "video_path = \"oak-d.avi\"\n",
    "# video_path = \"output.avi\" \n",
    "# video_path = \"traffic.mp4\"\n",
    "# video_path = \"street.mp4\"\n",
    "\n",
    "print(video_path)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(50) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.49 ðŸš€ Python-3.10.12 torch-2.5.1+cu124 CPU (13th Gen Intel Core(TM) i7-13700HX)\n",
      "Model summary (fused): 168 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'Tensorflow/workspace/datasets/yolo/yolov8n_voc.pt' with input shape (1, 3, 416, 416) BCHW and output shape(s) (1, 24, 3549) (5.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.1 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.45...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.7s, saved as 'Tensorflow/workspace/datasets/yolo/yolov8n_voc.onnx' (11.6 MB)\n",
      "\n",
      "Export complete (0.9s)\n",
      "Results saved to \u001b[1m/home/avncalst/Dropbox/deeplearning/SSD/RealTimeObjectDetection/Tensorflow/workspace/datasets/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=Tensorflow/workspace/datasets/yolo/yolov8n_voc.onnx imgsz=416  \n",
      "Validate:        yolo val task=detect model=Tensorflow/workspace/datasets/yolo/yolov8n_voc.onnx imgsz=416 data=/home/avncalst/.local/lib/python3.10/site-packages/ultralytics/cfg/datasets/VOC.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tensorflow/workspace/datasets/yolo/yolov8n_voc.onnx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_voc = YOLO(yolo_dir + \"/yolov8n_voc.pt\")\n",
    "model_voc.export(format='onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
